[2025-05-06T00:46:26.177+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-05-06T00:46:26.200+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tesla_stock_sentiment_final_etl.load_stock_data_to_snowflake scheduled__2025-05-04T02:30:00+00:00 [queued]>
[2025-05-06T00:46:26.212+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tesla_stock_sentiment_final_etl.load_stock_data_to_snowflake scheduled__2025-05-04T02:30:00+00:00 [queued]>
[2025-05-06T00:46:26.213+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 1
[2025-05-06T00:46:26.233+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): load_stock_data_to_snowflake> on 2025-05-04 02:30:00+00:00
[2025-05-06T00:46:26.245+0000] {standard_task_runner.py:72} INFO - Started process 698 to run task
[2025-05-06T00:46:26.257+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'tesla_stock_sentiment_final_etl', 'load_stock_data_to_snowflake', 'scheduled__2025-05-04T02:30:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/tesla_stock_etl.py', '--cfg-path', '/tmp/tmpa2i8ugsm']
[2025-05-06T00:46:26.270+0000] {standard_task_runner.py:105} INFO - Job 4: Subtask load_stock_data_to_snowflake
[2025-05-06T00:46:26.662+0000] {task_command.py:467} INFO - Running <TaskInstance: tesla_stock_sentiment_final_etl.load_stock_data_to_snowflake scheduled__2025-05-04T02:30:00+00:00 [running]> on host ccc68f7ecbda
[2025-05-06T00:46:26.903+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='tesla_stock_sentiment_final_etl' AIRFLOW_CTX_TASK_ID='load_stock_data_to_snowflake' AIRFLOW_CTX_EXECUTION_DATE='2025-05-04T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-05-04T02:30:00+00:00'
[2025-05-06T00:46:26.905+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-05-06T00:46:26.929+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/tesla_stock_etl.py", line 16, in load_stock_data_to_snowflake
    cur = return_snowflake_conn()
  File "/opt/airflow/dags/tesla_stock_etl.py", line 11, in return_snowflake_conn
    conn = hook.get_conn()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 299, in get_conn
    conn_config = self._get_conn_params
  File "/usr/local/lib/python3.9/functools.py", line 993, in __get__
    val = self.func(instance)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 192, in _get_conn_params
    conn = self.get_connection(self.snowflake_conn_id)  # type: ignore[attr-defined]
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/hooks/base.py", line 83, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/connection.py", line 537, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `snowflake_conn` isn't defined
[2025-05-06T00:46:26.940+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=tesla_stock_sentiment_final_etl, task_id=load_stock_data_to_snowflake, run_id=scheduled__2025-05-04T02:30:00+00:00, execution_date=20250504T023000, start_date=20250506T004626, end_date=20250506T004626
[2025-05-06T00:46:26.955+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-05-06T00:46:26.957+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 4 for task load_stock_data_to_snowflake (The conn_id `snowflake_conn` isn't defined; 698)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/tesla_stock_etl.py", line 16, in load_stock_data_to_snowflake
    cur = return_snowflake_conn()
  File "/opt/airflow/dags/tesla_stock_etl.py", line 11, in return_snowflake_conn
    conn = hook.get_conn()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 299, in get_conn
    conn_config = self._get_conn_params
  File "/usr/local/lib/python3.9/functools.py", line 993, in __get__
    val = self.func(instance)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 192, in _get_conn_params
    conn = self.get_connection(self.snowflake_conn_id)  # type: ignore[attr-defined]
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/hooks/base.py", line 83, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/connection.py", line 537, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `snowflake_conn` isn't defined
[2025-05-06T00:46:26.996+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-05-06T00:46:27.017+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-05-06T00:46:27.022+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-05-06T01:23:57.192+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-05-06T01:23:57.217+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tesla_stock_sentiment_final_etl.load_stock_data_to_snowflake scheduled__2025-05-04T02:30:00+00:00 [queued]>
[2025-05-06T01:23:57.231+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tesla_stock_sentiment_final_etl.load_stock_data_to_snowflake scheduled__2025-05-04T02:30:00+00:00 [queued]>
[2025-05-06T01:23:57.232+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 1
[2025-05-06T01:23:57.261+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): load_stock_data_to_snowflake> on 2025-05-04 02:30:00+00:00
[2025-05-06T01:23:57.278+0000] {standard_task_runner.py:72} INFO - Started process 2199 to run task
[2025-05-06T01:23:57.326+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'tesla_stock_sentiment_final_etl', 'load_stock_data_to_snowflake', 'scheduled__2025-05-04T02:30:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/tesla_stock_etl.py', '--cfg-path', '/tmp/tmpx6tqrjml']
[2025-05-06T01:23:57.336+0000] {standard_task_runner.py:105} INFO - Job 11: Subtask load_stock_data_to_snowflake
[2025-05-06T01:24:01.251+0000] {task_command.py:467} INFO - Running <TaskInstance: tesla_stock_sentiment_final_etl.load_stock_data_to_snowflake scheduled__2025-05-04T02:30:00+00:00 [running]> on host ccc68f7ecbda
[2025-05-06T01:24:01.392+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='tesla_stock_sentiment_final_etl' AIRFLOW_CTX_TASK_ID='load_stock_data_to_snowflake' AIRFLOW_CTX_EXECUTION_DATE='2025-05-04T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-05-04T02:30:00+00:00'
[2025-05-06T01:24:01.394+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-05-06T01:24:02.271+0000] {cursor.py:1156} INFO - Number of results in first chunk: 1
[2025-05-06T01:24:02.794+0000] {cursor.py:1156} INFO - Number of results in first chunk: 1
[2025-05-06T01:29:28.948+0000] {cursor.py:1156} INFO - Number of results in first chunk: 1
[2025-05-06T01:29:29.011+0000] {logging_mixin.py:190} INFO - Stock data (via Ticker.history) loaded into dev.raw.stock_data.
[2025-05-06T01:29:29.048+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-05-06T01:29:29.458+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-05-06T01:29:29.464+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=tesla_stock_sentiment_final_etl, task_id=load_stock_data_to_snowflake, run_id=scheduled__2025-05-04T02:30:00+00:00, execution_date=20250504T023000, start_date=20250506T012357, end_date=20250506T012929
[2025-05-06T01:29:29.697+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-05-06T01:29:29.837+0000] {taskinstance.py:3925} ERROR - Error scheduling downstream tasks. Skipping it as this is entirely optional optimisation. There might be various reasons for it, please take a look at the stack trace to figure out if the root cause can be diagnosed and fixed. See the issue https://github.com/apache/***/issues/39717 for details and an example problem. If you would like to get help in solving root cause, open discussion with all details with your managed service support or in Airflow repository.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3921, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3870, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py", line 2663, in partial_subset
    dag.task_dict = {
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py", line 2664, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py", line 2661, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/local/lib/python3.9/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1388, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/local/lib/python3.9/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/local/lib/python3.9/copy.py", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/local/lib/python3.9/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/local/lib/python3.9/copy.py", line 210, in _deepcopy_tuple
    y = [deepcopy(a, memo) for a in x]
  File "/usr/local/lib/python3.9/copy.py", line 210, in <listcomp>
    y = [deepcopy(a, memo) for a in x]
  File "/usr/local/lib/python3.9/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/local/lib/python3.9/copy.py", line 270, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/local/lib/python3.9/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/local/lib/python3.9/copy.py", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/local/lib/python3.9/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/local/lib/python3.9/copy.py", line 270, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/local/lib/python3.9/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/local/lib/python3.9/copy.py", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/local/lib/python3.9/copy.py", line 161, in deepcopy
    rv = reductor(4)
TypeError: cannot pickle '_thread.lock' object
[2025-05-06T01:29:29.880+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
