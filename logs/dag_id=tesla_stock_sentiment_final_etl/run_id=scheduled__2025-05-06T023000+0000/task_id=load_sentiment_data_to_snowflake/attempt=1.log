[2025-05-07T02:30:05.658+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-05-07T02:30:05.679+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tesla_stock_sentiment_final_etl.load_sentiment_data_to_snowflake scheduled__2025-05-06T02:30:00+00:00 [queued]>
[2025-05-07T02:30:05.690+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tesla_stock_sentiment_final_etl.load_sentiment_data_to_snowflake scheduled__2025-05-06T02:30:00+00:00 [queued]>
[2025-05-07T02:30:05.691+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 1
[2025-05-07T02:30:05.850+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): load_sentiment_data_to_snowflake> on 2025-05-06 02:30:00+00:00
[2025-05-07T02:30:05.861+0000] {standard_task_runner.py:72} INFO - Started process 4533 to run task
[2025-05-07T02:30:05.866+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'tesla_stock_sentiment_final_etl', 'load_sentiment_data_to_snowflake', 'scheduled__2025-05-06T02:30:00+00:00', '--job-id', '53', '--raw', '--subdir', 'DAGS_FOLDER/tesla_stock_etl.py', '--cfg-path', '/tmp/tmpk28a9re5']
[2025-05-07T02:30:05.869+0000] {standard_task_runner.py:105} INFO - Job 53: Subtask load_sentiment_data_to_snowflake
[2025-05-07T02:30:05.928+0000] {task_command.py:467} INFO - Running <TaskInstance: tesla_stock_sentiment_final_etl.load_sentiment_data_to_snowflake scheduled__2025-05-06T02:30:00+00:00 [running]> on host 55fd2231a72d
[2025-05-07T02:30:06.033+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='tesla_stock_sentiment_final_etl' AIRFLOW_CTX_TASK_ID='load_sentiment_data_to_snowflake' AIRFLOW_CTX_EXECUTION_DATE='2025-05-06T02:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-05-06T02:30:00+00:00'
[2025-05-07T02:30:06.035+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-05-07T02:30:06.492+0000] {cursor.py:1156} INFO - Number of results in first chunk: 1
[2025-05-07T02:30:06.801+0000] {cursor.py:1156} INFO - Number of results in first chunk: 1
[2025-05-07T02:43:49.371+0000] {cursor.py:1156} INFO - Number of results in first chunk: 1
[2025-05-07T02:43:49.497+0000] {logging_mixin.py:190} INFO - Sentiment data loaded into dev.raw.tweet_data.
[2025-05-07T02:43:49.527+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-05-07T02:43:49.914+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-05-07T02:43:49.923+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=tesla_stock_sentiment_final_etl, task_id=load_sentiment_data_to_snowflake, run_id=scheduled__2025-05-06T02:30:00+00:00, execution_date=20250506T023000, start_date=20250507T023005, end_date=20250507T024349
[2025-05-07T02:43:50.147+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-05-07T02:43:50.373+0000] {taskinstance.py:3925} ERROR - Error scheduling downstream tasks. Skipping it as this is entirely optional optimisation. There might be various reasons for it, please take a look at the stack trace to figure out if the root cause can be diagnosed and fixed. See the issue https://github.com/apache/***/issues/39717 for details and an example problem. If you would like to get help in solving root cause, open discussion with all details with your managed service support or in Airflow repository.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3921, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3870, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py", line 2663, in partial_subset
    dag.task_dict = {
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py", line 2664, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py", line 2661, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/local/lib/python3.9/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1388, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/local/lib/python3.9/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/local/lib/python3.9/copy.py", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/local/lib/python3.9/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/local/lib/python3.9/copy.py", line 210, in _deepcopy_tuple
    y = [deepcopy(a, memo) for a in x]
  File "/usr/local/lib/python3.9/copy.py", line 210, in <listcomp>
    y = [deepcopy(a, memo) for a in x]
  File "/usr/local/lib/python3.9/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/local/lib/python3.9/copy.py", line 270, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/local/lib/python3.9/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/local/lib/python3.9/copy.py", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/local/lib/python3.9/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/local/lib/python3.9/copy.py", line 270, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/local/lib/python3.9/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/local/lib/python3.9/copy.py", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/local/lib/python3.9/copy.py", line 161, in deepcopy
    rv = reductor(4)
TypeError: cannot pickle '_thread.lock' object
[2025-05-07T02:43:50.505+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
